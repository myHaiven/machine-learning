{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import pysptk\n",
    "from scipy.io import wavfile\n",
    "import parselmouth\n",
    "from parselmouth.praat import call\n",
    "import wave\n",
    "import statistics\n",
    "import keras\n",
    "#import pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hm_jit_shim(wav_file_path, f0min=75, f0max=300, unit=1):\n",
    "    sound = parselmouth.Sound(wav_file_path) # read the sound\n",
    "    #duration = call(sound, \"Get total duration\") # duration\n",
    "    pitch = call(sound, \"To Pitch\", 0.0, f0min, f0max) #create a praat pitch object\n",
    "    #meanF0 = call(pitch, \"Get mean\", 0, 0, unit) # get mean pitch\n",
    "    #stdevF0 = call(pitch, \"Get standard deviation\", 0 ,0, unit) # get standard deviation\n",
    "    harmonicity = call(sound, \"To Harmonicity (cc)\", 0.01, f0min, 0.1, 1.0)\n",
    "    hnr = call(harmonicity, \"Get mean\", 0, 0)\n",
    "    pointProcess = call(sound, \"To PointProcess (periodic, cc)\", f0min, f0max)\n",
    "    localJitter = call(pointProcess, \"Get jitter (local)\", 0, 0, 0.0001, 0.02, 1.3)\n",
    "    localabsoluteJitter = call(pointProcess, \"Get jitter (local, absolute)\", 0, 0, 0.0001, 0.02, 1.3)\n",
    "    rapJitter = call(pointProcess, \"Get jitter (rap)\", 0, 0, 0.0001, 0.02, 1.3)\n",
    "    ppq5Jitter = call(pointProcess, \"Get jitter (ppq5)\", 0, 0, 0.0001, 0.02, 1.3)\n",
    "    ddpJitter = call(pointProcess, \"Get jitter (ddp)\", 0, 0, 0.0001, 0.02, 1.3)\n",
    "    localShimmer =  call([sound, pointProcess], \"Get shimmer (local)\", 0, 0, 0.0001, 0.02, 1.3, 1.6)\n",
    "    localdbShimmer = call([sound, pointProcess], \"Get shimmer (local_dB)\", 0, 0, 0.0001, 0.02, 1.3, 1.6)\n",
    "    apq3Shimmer = call([sound, pointProcess], \"Get shimmer (apq3)\", 0, 0, 0.0001, 0.02, 1.3, 1.6)\n",
    "    aqpq5Shimmer = call([sound, pointProcess], \"Get shimmer (apq5)\", 0, 0, 0.0001, 0.02, 1.3, 1.6)\n",
    "    apq11Shimmer =  call([sound, pointProcess], \"Get shimmer (apq11)\", 0, 0, 0.0001, 0.02, 1.3, 1.6)\n",
    "    ddaShimmer = call([sound, pointProcess], \"Get shimmer (dda)\", 0, 0, 0.0001, 0.02, 1.3, 1.6)\n",
    "    \n",
    "    return [hnr, localJitter, localabsoluteJitter, rapJitter, ppq5Jitter, ddpJitter, localShimmer, localdbShimmer, apq3Shimmer, aqpq5Shimmer, apq11Shimmer, ddaShimmer]\n",
    "\n",
    "def get_formants(wav_file_path, f0min=75, f0max=300):\n",
    "    sound = parselmouth.Sound(wav_file_path) # read the sound\n",
    "    pitch = call(sound, \"To Pitch (cc)\", 0, f0min, 15, 'no', 0.03, 0.45, 0.01, 0.35, 0.14, f0max)\n",
    "    pointProcess = call(sound, \"To PointProcess (periodic, cc)\", f0min, f0max)\n",
    "    \n",
    "    formants = call(sound, \"To Formant (burg)\", 0.0025, 5, 5000, 0.025, 50)\n",
    "    numPoints = call(pointProcess, \"Get number of points\")\n",
    "\n",
    "    f1_list = []\n",
    "    f2_list = []\n",
    "    f3_list = []\n",
    "    f4_list = []\n",
    "    \n",
    "    # Measure formants only at glottal pulses\n",
    "    for point in range(0, numPoints):\n",
    "        point += 1\n",
    "        t = call(pointProcess, \"Get time from index\", point)\n",
    "        f1 = call(formants, \"Get value at time\", 1, t, 'Hertz', 'Linear')\n",
    "        f2 = call(formants, \"Get value at time\", 2, t, 'Hertz', 'Linear')\n",
    "        f3 = call(formants, \"Get value at time\", 3, t, 'Hertz', 'Linear')\n",
    "        f4 = call(formants, \"Get value at time\", 4, t, 'Hertz', 'Linear')\n",
    "        f1_list.append(f1)\n",
    "        f2_list.append(f2)\n",
    "        f3_list.append(f3)\n",
    "        f4_list.append(f4)\n",
    "    \n",
    "    f1_list = [f1 for f1 in f1_list if str(f1) != 'nan']\n",
    "    f2_list = [f2 for f2 in f2_list if str(f2) != 'nan']\n",
    "    f3_list = [f3 for f3 in f3_list if str(f3) != 'nan']\n",
    "    f4_list = [f4 for f4 in f4_list if str(f4) != 'nan']\n",
    "    \n",
    "    # calculate mean formants across pulses\n",
    "    f1_mean = statistics.mean(f1_list)\n",
    "    f2_mean = statistics.mean(f2_list)\n",
    "    f3_mean = statistics.mean(f3_list)\n",
    "    f4_mean = statistics.mean(f4_list)\n",
    "    \n",
    "    # calculate median formants across pulses, this is what is used in all subsequent calcualtions\n",
    "    # you can use mean if you want, just edit the code in the boxes below to replace median with mean\n",
    "    f1_median = statistics.median(f1_list)\n",
    "    f2_median = statistics.median(f2_list)\n",
    "    f3_median = statistics.median(f3_list)\n",
    "    f4_median = statistics.median(f4_list)\n",
    "    \n",
    "    return [f1_mean, f2_mean, f3_mean, f4_mean, f1_median, f2_median, f3_median, f4_median]\n",
    "\n",
    "\n",
    "PARSF = 120\n",
    "\n",
    "def get_aud_features_all(wav_path, num_mfcc = 40, hop_length = 512, n_fft = 2048, duration = 3, pad_mode = 'wrap'):\n",
    "    snd, r = librosa.load(wav_path, duration = 3)\n",
    "    #print(librosa.get_duration(filename = wav_path))\n",
    "    frames = r*duration #set default # of frames\n",
    "    if len(snd) < frames:\n",
    "        snd = np.pad(snd, frames-len(snd), mode = pad_mode)\n",
    "    snd = snd[:frames]\n",
    "    mfcc = librosa.feature.mfcc(snd, sr = r, n_mfcc = num_mfcc)\n",
    "    avg_mfcc = np.mean(mfcc, axis = 0)\n",
    "    #mel_freq_raw = librosa.feature.melspectrogram(snd, sr = r)\n",
    "    mel_raw = np.abs(librosa.stft(snd, n_fft = n_fft, hop_length = hop_length))\n",
    "    mel_freq = librosa.amplitude_to_db(mel_raw, ref = np.max)\n",
    "    avg_mel_freq = np.mean(mel_freq, axis = 0)\n",
    "    stft = np.abs(librosa.stft(snd)) #resolve complex values\n",
    "    chroma = librosa.feature.chroma_stft(S=stft, sr = r)\n",
    "    avg_chroma = np.mean(chroma, axis = 0)\n",
    "    oenv = librosa.onset.onset_strength(y = snd, sr=r, hop_length=hop_length)\n",
    "    tempogram = librosa.feature.tempogram(onset_envelope=oenv, sr=r, hop_length=512)\n",
    "    avg_tempogram = np.mean(tempogram, axis = 0)\n",
    "    pars_aud = parselmouth.Sound(wav_path)\n",
    "    intensity_obj = pars_aud.to_intensity()\n",
    "    intensity = intensity_obj.xs()\n",
    "    if len(intensity) < duration*PARSF:\n",
    "        intensity = np.pad(intensity, (0, duration*PARSF-len(intensity)), mode = pad_mode)\n",
    "        #print(intensity)\n",
    "    intensity = intensity[:duration*PARSF]\n",
    "    formants = get_formants(wav_path)\n",
    "    glottal = hm_jit_shim(wav_path)\n",
    "    #print(r, snd.shape, mfcc.shape, mel_freq.shape, chroma.shape, len(intensity), intensity.shape)#, tempogram.shape)\n",
    "     \n",
    "    return mfcc, avg_mfcc, mel_freq, avg_mel_freq, chroma, avg_chroma, intensity, formants, glottal, avg_tempogram\n",
    "\n",
    "def get_aud_features_vec(wav_path, num_mfcc = 40, hop_length = 512):\n",
    "    mfcc, avg_mfcc, mel_freq, avg_mel_freq, chroma, avg_chroma, intensity, formants, glottal, avg_tempogram = get_aud_features_all(wav_path, num_mfcc = num_mfcc, hop_length = hop_length)\n",
    "    return avg_mfcc, avg_mel_freq, avg_chroma, intensity, formants, glottal, avg_tempogram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aud_2d_feat(wav_path, num_mfcc = 40, hop_length = 512, n_fft = 2048, duration = 3, pad_mode = 'wrap'):\n",
    "    mfcc, avg_mfcc, mel_freq, avg_mel_freq, chroma, avg_chroma, intensity, formants, glottal, avg_tempogram = get_aud_features_all(wav_path, num_mfcc = num_mfcc, n_fft = n_fft, duration = duration, hop_length = hop_length, pad_mode = pad_mode)\n",
    "    mfcc = mfcc/240\n",
    "    mel_freq = mel_freq/(-80)\n",
    "    f_2d = np.concatenate((mfcc, mel_freq, chroma))\n",
    "    return f_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the dataframe\n",
    "\n",
    "std_len = 3\n",
    "frames = 22050\n",
    "\n",
    "SAVEE_path = \"AudioData/\"\n",
    "actors = [\"DC/\", \"JE/\", \"JK/\", \"KL/\"]\n",
    "emotions = [\"a\", \"d\", \"f\", \"h\", \"n\", \"sa\", \"su\"]\n",
    "n_samples = []\n",
    "\n",
    "for i in range(1, 16):\n",
    "    if i < 10:\n",
    "        num = \"0\"+str(i)\n",
    "    else:\n",
    "        num = str(i)\n",
    "    n_samples.append(num)\n",
    "\n",
    "features = []\n",
    "\n",
    "for i in actors:\n",
    "    for e in emotions:\n",
    "        for n in n_samples:\n",
    "            path = SAVEE_path+i+e+n+\".wav\"\n",
    "            #get_aud_features_all(path, hop_length=512, n_fft = 128)\n",
    "            #print(get_aud_2d_feat(path, hop_length=512, n_fft = 128))\n",
    "            #print(path)\n",
    "            if e == \"a\":\n",
    "                label = 1\n",
    "            else:\n",
    "                label = 0\n",
    "            data = get_aud_2d_feat(path, hop_length=512, n_fft = 128)\n",
    "            features.append([data, label])\n",
    "        \n",
    "SAVEEdf = pd.DataFrame(features, columns = [\"feature\", \"class_label\"])\n",
    "#print(SAVEEdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Convert features and corresponding classification labels into numpy arrays\n",
    "X = np.array(SAVEEdf.feature.tolist())\n",
    "y = np.array(SAVEEdf.class_label.tolist())\n",
    "\n",
    "# Encode the classification labels\n",
    "le = LabelEncoder()\n",
    "yy = to_categorical(le.fit_transform(y)) \n",
    "\n",
    "# split the dataset \n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, yy, test_size=0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(336, 117, 130)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from sklearn import metrics \n",
    "\n",
    "num_rows = 117\n",
    "num_columns = 130\n",
    "num_channels = 1\n",
    "\n",
    "#print(x_train.shape)\n",
    "x_train = x_train.reshape(x_train.shape[0], num_rows, num_columns, num_channels)\n",
    "x_test = x_test.reshape(x_test.shape[0], num_rows, num_columns, num_channels)\n",
    "\n",
    "num_labels = yy.shape[1]\n",
    "filter_size = 2\n",
    "\n",
    "# Construct model \n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=16, kernel_size=2, input_shape=(num_rows, num_columns, num_channels), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(GlobalAveragePooling2D())\n",
    "\n",
    "model.add(Dense(num_labels, activation='softmax'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 116, 129, 16)      80        \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 58, 64, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 58, 64, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 57, 63, 32)        2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 28, 31, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 28, 31, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 27, 30, 128)       16512     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 13, 15, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 13, 15, 128)       0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 18,930\n",
      "Trainable params: 18,930\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "3/3 [==============================] - 0s 64ms/step - loss: 0.6781 - accuracy: 0.9048\n",
      "Pre-training accuracy: 90.4762%\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "\n",
    "# Display model architecture summary \n",
    "model.summary()\n",
    "\n",
    "# Calculate pre-training accuracy \n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "accuracy = 100*score[1]\n",
    "\n",
    "print(\"Pre-training accuracy: %.4f%%\" % accuracy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/72\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.6697 - accuracy: 0.8452\n",
      "Epoch 00001: val_loss improved from inf to 0.60419, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "2/2 [==============================] - 2s 941ms/step - loss: 0.6697 - accuracy: 0.8452 - val_loss: 0.6042 - val_accuracy: 0.9048\n",
      "Epoch 2/72\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.5636 - accuracy: 0.8452\n",
      "Epoch 00002: val_loss improved from 0.60419 to 0.53832, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "2/2 [==============================] - 2s 850ms/step - loss: 0.5636 - accuracy: 0.8452 - val_loss: 0.5383 - val_accuracy: 0.9048\n",
      "Epoch 3/72\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4930 - accuracy: 0.8452\n",
      "Epoch 00003: val_loss improved from 0.53832 to 0.47748, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "2/2 [==============================] - 2s 881ms/step - loss: 0.4930 - accuracy: 0.8452 - val_loss: 0.4775 - val_accuracy: 0.9048\n",
      "Epoch 4/72\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4467 - accuracy: 0.8452\n",
      "Epoch 00004: val_loss improved from 0.47748 to 0.42454, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "2/2 [==============================] - 2s 1s/step - loss: 0.4467 - accuracy: 0.8452 - val_loss: 0.4245 - val_accuracy: 0.9048\n",
      "Epoch 5/72\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4281 - accuracy: 0.8452\n",
      "Epoch 00005: val_loss improved from 0.42454 to 0.38435, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "2/2 [==============================] - 2s 813ms/step - loss: 0.4281 - accuracy: 0.8452 - val_loss: 0.3844 - val_accuracy: 0.9048\n",
      "Epoch 6/72\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4351 - accuracy: 0.8452\n",
      "Epoch 00006: val_loss improved from 0.38435 to 0.36233, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "2/2 [==============================] - 1s 710ms/step - loss: 0.4351 - accuracy: 0.8452 - val_loss: 0.3623 - val_accuracy: 0.9048\n",
      "Epoch 7/72\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4513 - accuracy: 0.8452\n",
      "Epoch 00007: val_loss improved from 0.36233 to 0.35703, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "2/2 [==============================] - 1s 695ms/step - loss: 0.4513 - accuracy: 0.8452 - val_loss: 0.3570 - val_accuracy: 0.9048\n",
      "Epoch 8/72\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4515 - accuracy: 0.8452\n",
      "Epoch 00008: val_loss did not improve from 0.35703\n",
      "2/2 [==============================] - 1s 717ms/step - loss: 0.4515 - accuracy: 0.8452 - val_loss: 0.3656 - val_accuracy: 0.9048\n",
      "Epoch 9/72\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4430 - accuracy: 0.8452\n",
      "Epoch 00009: val_loss did not improve from 0.35703\n",
      "2/2 [==============================] - 2s 794ms/step - loss: 0.4430 - accuracy: 0.8452 - val_loss: 0.3812 - val_accuracy: 0.9048\n",
      "Epoch 10/72\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4349 - accuracy: 0.8452\n",
      "Epoch 00010: val_loss did not improve from 0.35703\n",
      "2/2 [==============================] - 1s 739ms/step - loss: 0.4349 - accuracy: 0.8452 - val_loss: 0.3991 - val_accuracy: 0.9048\n",
      "Epoch 11/72\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4289 - accuracy: 0.8452\n",
      "Epoch 00011: val_loss did not improve from 0.35703\n",
      "2/2 [==============================] - 1s 713ms/step - loss: 0.4289 - accuracy: 0.8452 - val_loss: 0.4156 - val_accuracy: 0.9048\n",
      "Epoch 12/72\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4293 - accuracy: 0.8452\n",
      "Epoch 00012: val_loss did not improve from 0.35703\n",
      "2/2 [==============================] - 1s 702ms/step - loss: 0.4293 - accuracy: 0.8452 - val_loss: 0.4296 - val_accuracy: 0.9048\n",
      "Epoch 13/72\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4311 - accuracy: 0.8452\n",
      "Epoch 00013: val_loss did not improve from 0.35703\n",
      "2/2 [==============================] - 2s 806ms/step - loss: 0.4311 - accuracy: 0.8452 - val_loss: 0.4387 - val_accuracy: 0.9048\n",
      "Epoch 14/72\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4329 - accuracy: 0.8452\n",
      "Epoch 00014: val_loss did not improve from 0.35703\n",
      "2/2 [==============================] - 2s 917ms/step - loss: 0.4329 - accuracy: 0.8452 - val_loss: 0.4433 - val_accuracy: 0.9048\n",
      "Epoch 15/72\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4342 - accuracy: 0.8452\n",
      "Epoch 00015: val_loss did not improve from 0.35703\n",
      "2/2 [==============================] - 2s 805ms/step - loss: 0.4342 - accuracy: 0.8452 - val_loss: 0.4447 - val_accuracy: 0.9048\n",
      "Epoch 16/72\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4348 - accuracy: 0.8452\n",
      "Epoch 00016: val_loss did not improve from 0.35703\n",
      "2/2 [==============================] - 2s 874ms/step - loss: 0.4348 - accuracy: 0.8452 - val_loss: 0.4412 - val_accuracy: 0.9048\n",
      "Epoch 17/72\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4345 - accuracy: 0.8452\n",
      "Epoch 00017: val_loss did not improve from 0.35703\n",
      "2/2 [==============================] - 2s 793ms/step - loss: 0.4345 - accuracy: 0.8452 - val_loss: 0.4337 - val_accuracy: 0.9048\n",
      "Epoch 18/72\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4322 - accuracy: 0.8452\n",
      "Epoch 00018: val_loss did not improve from 0.35703\n",
      "2/2 [==============================] - 2s 796ms/step - loss: 0.4322 - accuracy: 0.8452 - val_loss: 0.4256 - val_accuracy: 0.9048\n",
      "Epoch 19/72\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4305 - accuracy: 0.8452\n",
      "Epoch 00019: val_loss did not improve from 0.35703\n",
      "2/2 [==============================] - 2s 795ms/step - loss: 0.4305 - accuracy: 0.8452 - val_loss: 0.4167 - val_accuracy: 0.9048\n",
      "Epoch 20/72\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4303 - accuracy: 0.8452\n",
      "Epoch 00020: val_loss did not improve from 0.35703\n",
      "2/2 [==============================] - 2s 823ms/step - loss: 0.4303 - accuracy: 0.8452 - val_loss: 0.4078 - val_accuracy: 0.9048\n",
      "Epoch 21/72\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4293 - accuracy: 0.8452\n",
      "Epoch 00021: val_loss did not improve from 0.35703\n",
      "2/2 [==============================] - 2s 968ms/step - loss: 0.4293 - accuracy: 0.8452 - val_loss: 0.4030 - val_accuracy: 0.9048\n",
      "Epoch 22/72\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4285 - accuracy: 0.8452\n",
      "Epoch 00022: val_loss did not improve from 0.35703\n",
      "2/2 [==============================] - 1s 736ms/step - loss: 0.4285 - accuracy: 0.8452 - val_loss: 0.3996 - val_accuracy: 0.9048\n",
      "Epoch 23/72\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4287 - accuracy: 0.8452\n",
      "Epoch 00023: val_loss did not improve from 0.35703\n",
      "2/2 [==============================] - 1s 744ms/step - loss: 0.4287 - accuracy: 0.8452 - val_loss: 0.3981 - val_accuracy: 0.9048\n",
      "Epoch 24/72\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4286 - accuracy: 0.8452\n",
      "Epoch 00024: val_loss did not improve from 0.35703\n",
      "2/2 [==============================] - 2s 768ms/step - loss: 0.4286 - accuracy: 0.8452 - val_loss: 0.3984 - val_accuracy: 0.9048\n",
      "Epoch 25/72\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4282 - accuracy: 0.8452\n",
      "Epoch 00025: val_loss did not improve from 0.35703\n",
      "2/2 [==============================] - 1s 750ms/step - loss: 0.4282 - accuracy: 0.8452 - val_loss: 0.3996 - val_accuracy: 0.9048\n",
      "Epoch 26/72\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4291 - accuracy: 0.8452\n",
      "Epoch 00026: val_loss did not improve from 0.35703\n",
      "2/2 [==============================] - 2s 775ms/step - loss: 0.4291 - accuracy: 0.8452 - val_loss: 0.4019 - val_accuracy: 0.9048\n",
      "Epoch 27/72\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4283 - accuracy: 0.8452\n",
      "Epoch 00027: val_loss did not improve from 0.35703\n",
      "2/2 [==============================] - 2s 952ms/step - loss: 0.4283 - accuracy: 0.8452 - val_loss: 0.4020 - val_accuracy: 0.9048\n",
      "Epoch 28/72\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4281 - accuracy: 0.8452\n",
      "Epoch 00028: val_loss did not improve from 0.35703\n",
      "2/2 [==============================] - 2s 786ms/step - loss: 0.4281 - accuracy: 0.8452 - val_loss: 0.4027 - val_accuracy: 0.9048\n",
      "Epoch 29/72\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4282 - accuracy: 0.8452\n",
      "Epoch 00029: val_loss did not improve from 0.35703\n",
      "2/2 [==============================] - 2s 783ms/step - loss: 0.4282 - accuracy: 0.8452 - val_loss: 0.4035 - val_accuracy: 0.9048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/72\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4283 - accuracy: 0.8452\n",
      "Epoch 00030: val_loss did not improve from 0.35703\n",
      "2/2 [==============================] - 1s 749ms/step - loss: 0.4283 - accuracy: 0.8452 - val_loss: 0.4065 - val_accuracy: 0.9048\n",
      "Epoch 31/72\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4284 - accuracy: 0.8452\n",
      "Epoch 00031: val_loss did not improve from 0.35703\n",
      "2/2 [==============================] - 1s 737ms/step - loss: 0.4284 - accuracy: 0.8452 - val_loss: 0.4082 - val_accuracy: 0.9048\n",
      "Epoch 32/72\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4291 - accuracy: 0.8452\n",
      "Epoch 00032: val_loss did not improve from 0.35703\n",
      "2/2 [==============================] - 1s 738ms/step - loss: 0.4291 - accuracy: 0.8452 - val_loss: 0.4095 - val_accuracy: 0.9048\n",
      "Epoch 33/72\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4286 - accuracy: 0.8452\n",
      "Epoch 00033: val_loss did not improve from 0.35703\n",
      "2/2 [==============================] - 1s 749ms/step - loss: 0.4286 - accuracy: 0.8452 - val_loss: 0.4081 - val_accuracy: 0.9048\n",
      "Epoch 34/72\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4288 - accuracy: 0.8452\n",
      "Epoch 00034: val_loss did not improve from 0.35703\n",
      "2/2 [==============================] - 1s 732ms/step - loss: 0.4288 - accuracy: 0.8452 - val_loss: 0.4042 - val_accuracy: 0.9048\n",
      "Epoch 35/72\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4282 - accuracy: 0.8452\n",
      "Epoch 00035: val_loss did not improve from 0.35703\n",
      "2/2 [==============================] - 1s 732ms/step - loss: 0.4282 - accuracy: 0.8452 - val_loss: 0.4027 - val_accuracy: 0.9048\n",
      "Epoch 36/72\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4281 - accuracy: 0.8452\n",
      "Epoch 00036: val_loss did not improve from 0.35703\n",
      "2/2 [==============================] - 1s 721ms/step - loss: 0.4281 - accuracy: 0.8452 - val_loss: 0.4030 - val_accuracy: 0.9048\n",
      "Epoch 37/72\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4285 - accuracy: 0.8452\n",
      "Epoch 00037: val_loss did not improve from 0.35703\n",
      "2/2 [==============================] - 1s 742ms/step - loss: 0.4285 - accuracy: 0.8452 - val_loss: 0.4025 - val_accuracy: 0.9048\n",
      "Epoch 38/72\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4281 - accuracy: 0.8452\n",
      "Epoch 00038: val_loss did not improve from 0.35703\n",
      "2/2 [==============================] - 1s 735ms/step - loss: 0.4281 - accuracy: 0.8452 - val_loss: 0.4001 - val_accuracy: 0.9048\n",
      "Epoch 39/72\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4277 - accuracy: 0.8452\n",
      "Epoch 00039: val_loss did not improve from 0.35703\n",
      "2/2 [==============================] - 1s 731ms/step - loss: 0.4277 - accuracy: 0.8452 - val_loss: 0.3976 - val_accuracy: 0.9048\n",
      "Epoch 40/72\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4283 - accuracy: 0.8452\n",
      "Epoch 00040: val_loss did not improve from 0.35703\n",
      "2/2 [==============================] - 2s 774ms/step - loss: 0.4283 - accuracy: 0.8452 - val_loss: 0.3938 - val_accuracy: 0.9048\n",
      "Epoch 41/72\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4271 - accuracy: 0.8452\n",
      "Epoch 00041: val_loss did not improve from 0.35703\n",
      "2/2 [==============================] - 2s 767ms/step - loss: 0.4271 - accuracy: 0.8452 - val_loss: 0.3917 - val_accuracy: 0.9048\n",
      "Epoch 42/72\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4281 - accuracy: 0.8452\n",
      "Epoch 00042: val_loss did not improve from 0.35703\n",
      "2/2 [==============================] - 2s 775ms/step - loss: 0.4281 - accuracy: 0.8452 - val_loss: 0.3911 - val_accuracy: 0.9048\n",
      "Epoch 43/72\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4289 - accuracy: 0.8452\n",
      "Epoch 00043: val_loss did not improve from 0.35703\n",
      "2/2 [==============================] - 2s 760ms/step - loss: 0.4289 - accuracy: 0.8452 - val_loss: 0.3937 - val_accuracy: 0.9048\n",
      "Epoch 44/72\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4278 - accuracy: 0.8452\n",
      "Epoch 00044: val_loss did not improve from 0.35703\n",
      "2/2 [==============================] - 1s 747ms/step - loss: 0.4278 - accuracy: 0.8452 - val_loss: 0.3987 - val_accuracy: 0.9048\n",
      "Epoch 45/72\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4272 - accuracy: 0.8452\n",
      "Epoch 00045: val_loss did not improve from 0.35703\n",
      "2/2 [==============================] - 1s 737ms/step - loss: 0.4272 - accuracy: 0.8452 - val_loss: 0.4040 - val_accuracy: 0.9048\n",
      "Epoch 46/72\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4278 - accuracy: 0.8452\n",
      "Epoch 00046: val_loss did not improve from 0.35703\n",
      "2/2 [==============================] - 1s 734ms/step - loss: 0.4278 - accuracy: 0.8452 - val_loss: 0.4089 - val_accuracy: 0.9048\n",
      "Epoch 47/72\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4277 - accuracy: 0.8452\n",
      "Epoch 00047: val_loss did not improve from 0.35703\n",
      "2/2 [==============================] - 2s 838ms/step - loss: 0.4277 - accuracy: 0.8452 - val_loss: 0.4139 - val_accuracy: 0.9048\n",
      "Epoch 48/72\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4284 - accuracy: 0.8452\n",
      "Epoch 00048: val_loss did not improve from 0.35703\n",
      "2/2 [==============================] - 2s 790ms/step - loss: 0.4284 - accuracy: 0.8452 - val_loss: 0.4168 - val_accuracy: 0.9048\n",
      "Epoch 49/72\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4284 - accuracy: 0.8452\n",
      "Epoch 00049: val_loss did not improve from 0.35703\n",
      "2/2 [==============================] - 1s 731ms/step - loss: 0.4284 - accuracy: 0.8452 - val_loss: 0.4144 - val_accuracy: 0.9048\n",
      "Epoch 50/72\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4276 - accuracy: 0.8452\n",
      "Epoch 00050: val_loss did not improve from 0.35703\n",
      "2/2 [==============================] - 2s 837ms/step - loss: 0.4276 - accuracy: 0.8452 - val_loss: 0.4090 - val_accuracy: 0.9048\n",
      "Epoch 51/72\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4265 - accuracy: 0.8452\n",
      "Epoch 00051: val_loss did not improve from 0.35703\n",
      "2/2 [==============================] - 2s 788ms/step - loss: 0.4265 - accuracy: 0.8452 - val_loss: 0.4013 - val_accuracy: 0.9048\n",
      "Epoch 52/72\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4264 - accuracy: 0.8452\n",
      "Epoch 00052: val_loss did not improve from 0.35703\n",
      "2/2 [==============================] - 2s 756ms/step - loss: 0.4264 - accuracy: 0.8452 - val_loss: 0.3929 - val_accuracy: 0.9048\n",
      "Epoch 53/72\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4264 - accuracy: 0.8452\n",
      "Epoch 00053: val_loss did not improve from 0.35703\n",
      "2/2 [==============================] - 1s 732ms/step - loss: 0.4264 - accuracy: 0.8452 - val_loss: 0.3853 - val_accuracy: 0.9048\n",
      "Epoch 54/72\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4271 - accuracy: 0.8452\n",
      "Epoch 00054: val_loss did not improve from 0.35703\n",
      "2/2 [==============================] - 1s 746ms/step - loss: 0.4271 - accuracy: 0.8452 - val_loss: 0.3805 - val_accuracy: 0.9048\n",
      "Epoch 55/72\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4290 - accuracy: 0.8452\n",
      "Epoch 00055: val_loss did not improve from 0.35703\n",
      "2/2 [==============================] - 2s 770ms/step - loss: 0.4290 - accuracy: 0.8452 - val_loss: 0.3795 - val_accuracy: 0.9048\n",
      "Epoch 56/72\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4277 - accuracy: 0.8452\n",
      "Epoch 00056: val_loss did not improve from 0.35703\n",
      "2/2 [==============================] - 1s 699ms/step - loss: 0.4277 - accuracy: 0.8452 - val_loss: 0.3852 - val_accuracy: 0.9048\n",
      "Epoch 57/72\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4268 - accuracy: 0.8452\n",
      "Epoch 00057: val_loss did not improve from 0.35703\n",
      "2/2 [==============================] - 1s 714ms/step - loss: 0.4268 - accuracy: 0.8452 - val_loss: 0.3920 - val_accuracy: 0.9048\n",
      "Epoch 58/72\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4245 - accuracy: 0.8452\n",
      "Epoch 00058: val_loss did not improve from 0.35703\n",
      "2/2 [==============================] - 2s 863ms/step - loss: 0.4245 - accuracy: 0.8452 - val_loss: 0.4020 - val_accuracy: 0.9048\n",
      "Epoch 59/72\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4265 - accuracy: 0.8452\n",
      "Epoch 00059: val_loss did not improve from 0.35703\n",
      "2/2 [==============================] - 1s 747ms/step - loss: 0.4265 - accuracy: 0.8452 - val_loss: 0.4123 - val_accuracy: 0.9048\n",
      "Epoch 60/72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - ETA: 0s - loss: 0.4269 - accuracy: 0.8452\n",
      "Epoch 00060: val_loss did not improve from 0.35703\n",
      "2/2 [==============================] - 2s 778ms/step - loss: 0.4269 - accuracy: 0.8452 - val_loss: 0.4147 - val_accuracy: 0.9048\n",
      "Epoch 61/72\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4268 - accuracy: 0.8452\n",
      "Epoch 00061: val_loss did not improve from 0.35703\n",
      "2/2 [==============================] - 2s 777ms/step - loss: 0.4268 - accuracy: 0.8452 - val_loss: 0.4112 - val_accuracy: 0.9048\n",
      "Epoch 62/72\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4261 - accuracy: 0.8452\n",
      "Epoch 00062: val_loss did not improve from 0.35703\n",
      "2/2 [==============================] - 2s 762ms/step - loss: 0.4261 - accuracy: 0.8452 - val_loss: 0.4037 - val_accuracy: 0.9048\n",
      "Epoch 63/72\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4254 - accuracy: 0.8452\n",
      "Epoch 00063: val_loss did not improve from 0.35703\n",
      "2/2 [==============================] - 1s 718ms/step - loss: 0.4254 - accuracy: 0.8452 - val_loss: 0.3945 - val_accuracy: 0.9048\n",
      "Epoch 64/72\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4241 - accuracy: 0.8452\n",
      "Epoch 00064: val_loss did not improve from 0.35703\n",
      "2/2 [==============================] - 1s 742ms/step - loss: 0.4241 - accuracy: 0.8452 - val_loss: 0.3892 - val_accuracy: 0.9048\n",
      "Epoch 65/72\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4237 - accuracy: 0.8452\n",
      "Epoch 00065: val_loss did not improve from 0.35703\n",
      "2/2 [==============================] - 1s 744ms/step - loss: 0.4237 - accuracy: 0.8452 - val_loss: 0.3878 - val_accuracy: 0.9048\n",
      "Epoch 66/72\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4233 - accuracy: 0.8452\n",
      "Epoch 00066: val_loss did not improve from 0.35703\n",
      "2/2 [==============================] - 1s 737ms/step - loss: 0.4233 - accuracy: 0.8452 - val_loss: 0.3873 - val_accuracy: 0.9048\n",
      "Epoch 67/72\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4235 - accuracy: 0.8452\n",
      "Epoch 00067: val_loss did not improve from 0.35703\n",
      "2/2 [==============================] - 1s 729ms/step - loss: 0.4235 - accuracy: 0.8452 - val_loss: 0.3849 - val_accuracy: 0.9048\n",
      "Epoch 68/72\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4226 - accuracy: 0.8452\n",
      "Epoch 00068: val_loss did not improve from 0.35703\n",
      "2/2 [==============================] - 1s 724ms/step - loss: 0.4226 - accuracy: 0.8452 - val_loss: 0.3817 - val_accuracy: 0.9048\n",
      "Epoch 69/72\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4219 - accuracy: 0.8452\n",
      "Epoch 00069: val_loss did not improve from 0.35703\n",
      "2/2 [==============================] - 1s 724ms/step - loss: 0.4219 - accuracy: 0.8452 - val_loss: 0.3786 - val_accuracy: 0.9048\n",
      "Epoch 70/72\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4222 - accuracy: 0.8452\n",
      "Epoch 00070: val_loss did not improve from 0.35703\n",
      "2/2 [==============================] - 2s 935ms/step - loss: 0.4222 - accuracy: 0.8452 - val_loss: 0.3779 - val_accuracy: 0.9048\n",
      "Epoch 71/72\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4224 - accuracy: 0.8452\n",
      "Epoch 00071: val_loss did not improve from 0.35703\n",
      "2/2 [==============================] - 1s 744ms/step - loss: 0.4224 - accuracy: 0.8452 - val_loss: 0.3789 - val_accuracy: 0.9048\n",
      "Epoch 72/72\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.4215 - accuracy: 0.8452\n",
      "Epoch 00072: val_loss did not improve from 0.35703\n",
      "2/2 [==============================] - 2s 766ms/step - loss: 0.4215 - accuracy: 0.8452 - val_loss: 0.3826 - val_accuracy: 0.9048\n",
      "Training completed in time:  0:06:30.326183\n",
      "Training Accuracy:  0.8452380895614624\n",
      "Testing Accuracy:  0.9047619104385376\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint \n",
    "from datetime import datetime \n",
    "\n",
    "num_epochs = 72\n",
    "num_batch_size = 256\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.basic_cnn.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "start = datetime.now()\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(x_test, y_test), callbacks=[checkpointer], verbose=1)\n",
    "\n",
    "\n",
    "duration = datetime.now() - start\n",
    "print(\"Training completed in time: \", duration)\n",
    "\n",
    "\n",
    "# Evaluating the model on the training and testing set\n",
    "score = model.evaluate(x_train, y_train, verbose=0)\n",
    "print(\"Training Accuracy: \", score[1])\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Testing Accuracy: \", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
